{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af70ed79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T10:04:56.458810Z",
     "start_time": "2024-07-16T10:04:56.449768Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from microtorch import Tensor\n",
    "from microtorch import nn\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8d13770-c74a-4519-b0e6-0722d53ccdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print (torch.__version__)\n",
    "\n",
    "_ = torch.manual_seed (2024)\n",
    "\n",
    "def softmax_grad(probs):\n",
    "    tensor = probs.clone().detach()\n",
    "    flat = torch.flatten(tensor)\n",
    "    diagonal = torch.diagflat(flat)\n",
    "    off_diagonal = torch.outer(flat, flat)\n",
    "    return diagonal - off_diagonal\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4384252-e783-4556-aef7-83cbf92b4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = ...\n",
      "tensor([0., 1., 2., 3.])\n",
      "probs_y = ...\n",
      "tensor([0.0321, 0.0871, 0.2369, 0.6439])\n",
      "jacyA = ...\n",
      "tensor([[ 0.0310, -0.0028, -0.0076, -0.0206],\n",
      "        [-0.0028,  0.0796, -0.0206, -0.0561],\n",
      "        [-0.0076, -0.0206,  0.1808, -0.1525],\n",
      "        [-0.0206, -0.0561, -0.1525,  0.2293]])\n",
      "torch.allclose (jacyA, jacyB) = True\n"
     ]
    }
   ],
   "source": [
    "y = torch.arange (4.)                            # no batch dimension\n",
    "probs_y = y.softmax (dim = -1)\n",
    "\n",
    "print ('y = ...')\n",
    "print (y)\n",
    "print ('probs_y = ...')\n",
    "print (probs_y)\n",
    "\n",
    "jacyA = softmax_grad (probs_y)                   # works with no batch dimension\n",
    "jacyB = torch.autograd.functional.jacobian (torch.nn.Softmax (dim = -1), y)\n",
    "\n",
    "print ('jacyA = ...')\n",
    "print (jacyA)\n",
    "print ('torch.allclose (jacyA, jacyB) =', torch.allclose (jacyA, jacyB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f01aca3c-46d2-47b8-829f-7ac7c7613191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = ...\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]], requires_grad=True)\n",
      "probs_x = ...\n",
      "tensor([[0.0321, 0.0871, 0.2369, 0.6439],\n",
      "        [0.0321, 0.0871, 0.2369, 0.6439],\n",
      "        [0.0321, 0.0871, 0.2369, 0.6439],\n",
      "        [0.0321, 0.0871, 0.2369, 0.6439],\n",
      "        [0.0321, 0.0871, 0.2369, 0.6439]], grad_fn=<SoftmaxBackward0>)\n",
      "jacxA.shape = torch.Size([20, 20])\n",
      "jacxB.shape = torch.Size([5, 4, 5, 4])\n",
      "(jfA - jfB).abs().max() = tensor(0.4146)\n",
      "(jfA == 0).sum() = tensor(0)\n",
      "(jfB == 0).sum() = tensor(320)\n",
      "torch.allclose (jfA[torch.where (jfB != 0)], jfB[torch.where (jfB != 0)]) = True\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange (20).reshape (5, 4)             # has batch dimension\n",
    "x = x.float().requires_grad_()\n",
    "probs_x = x.softmax (dim = -1)\n",
    "\n",
    "print ('x = ...')\n",
    "print (x)\n",
    "print ('probs_x = ...')\n",
    "print (probs_x)\n",
    "\n",
    "jacxA = softmax_grad (probs_x)                   # incorrectly flattens batch and features dimensions together\n",
    "jacxB = torch.autograd.functional.jacobian (torch.nn.Softmax (dim = -1), x)\n",
    "\n",
    "print ('jacxA.shape =', jacxA.shape)             # wrong shape for jacobian of 2d tensor\n",
    "print ('jacxB.shape =', jacxB.shape)\n",
    "\n",
    "jfA = jacxA.flatten()                            # reshaping doesn't help\n",
    "jfB = jacxB.flatten()\n",
    "print ('(jfA - jfB).abs().max() =', (jfA - jfB).abs().max())\n",
    "\n",
    "print ('(jfA == 0).sum() =', (jfA == 0).sum())   # too many non-zero elements (but ...)\n",
    "print ('(jfB == 0).sum() =', (jfB == 0).sum())\n",
    "\n",
    "print ('torch.allclose (jfA[torch.where (jfB != 0)], jfB[torch.where (jfB != 0)]) =', torch.allclose (jfA[torch.where (jfB != 0)], jfB[torch.where (jfB != 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14e4d1f5-aa3d-45cb-b870-88b8d7b5a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = ...\n",
      "tensor([[-1.2262, -0.0093,  1.5420, -0.4657],\n",
      "        [ 1.8567,  1.9776, -0.4322,  1.3667],\n",
      "        [ 0.7131, -0.3869, -0.2535, -1.6675],\n",
      "        [ 0.9962,  0.9391,  1.4148,  0.6343],\n",
      "        [-0.0776, -1.1175, -0.6481,  0.6530]], requires_grad=True)\n",
      "probs_z = ...\n",
      "tensor([[0.0446, 0.1504, 0.7097, 0.0953],\n",
      "        [0.3518, 0.3970, 0.0357, 0.2155],\n",
      "        [0.5538, 0.1843, 0.2107, 0.0512],\n",
      "        [0.2403, 0.2270, 0.3653, 0.1674],\n",
      "        [0.2503, 0.0885, 0.1415, 0.5197]], grad_fn=<SoftmaxBackward0>)\n",
      "probs_z.sum (dim = -1) = ...\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "gradzA = ...\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [6.6016e-08, 2.1975e-08, 2.5112e-08, 6.1062e-09],\n",
      "        [1.4326e-08, 1.3531e-08, 2.1772e-08, 9.9763e-09],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "torch.allclose (gradzA, gradzB, atol = 1.e-7) = True\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn (5, 4, requires_grad = True)     # has batch dimension\n",
    "\n",
    "probs_z = z.softmax (dim = -1)\n",
    "\n",
    "print ('z = ...')\n",
    "print (z)\n",
    "print ('probs_z = ...')\n",
    "print (probs_z)\n",
    "print ('probs_z.sum (dim = -1) = ...')\n",
    "print (probs_z.sum (dim = -1))                   # probs sum to one, a constant, for each row in batch\n",
    "\n",
    "probs_z.sum().backward()                         # sum over batch, as well as features, to get a scalar\n",
    "gradzA = z.grad                                  # gradient of sum of softmax is zero because sum is constant\n",
    "jaczB = torch.autograd.functional.jacobian (torch.nn.Softmax (dim = -1), z)\n",
    "gradzB = jaczB.sum (dim = (-2, -1))              # sum of partials (jacobian) is derivative (gradient) of sum\n",
    "\n",
    "print ('gradzA = ...')\n",
    "print (gradzA)\n",
    "print ('torch.allclose (gradzA, gradzB, atol = 1.e-7) =', torch.allclose (gradzA, gradzB, atol = 1.e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ce9784d3-e9e3-44f3-a98b-3d814fb395fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(t: Tensor, dim: int = -1):\n",
    "    m = t.data.max()\n",
    "    e = np.exp(t.data - m)\n",
    "    s = e / e.sum(axis=dim, keepdims=True)\n",
    "    out = Tensor(s, _children=(t,), _op='softmax')\n",
    "\n",
    "    def softmax_backward():\n",
    "        for i in range(len(s)):\n",
    "            si = s[i].reshape(1, -1)\n",
    "            j = np.diagflat(si) - si.T@si  # jacobian matrix\n",
    "            _j = np.concatenate((j, np.zeros_like(j)), axis=-1).reshape(4, 2, 4)\n",
    "            print(_j.shape)\n",
    "            print(_j.sum(axis=0))\n",
    "            t.grad += out.grad@j\n",
    "        \n",
    "    out._backward = softmax_backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "9c196b3e-6a18-40ae-8c8e-3d9fba24206c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transpose() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[281], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: transpose() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "np.transpose(np.ones((2, 3, 4)), axis=[1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "eb5dff06-0d30-47b9-8c68-5eb4f978f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(t: Tensor, dim: int = -1):\n",
    "    m = t.data.max()\n",
    "    e = np.exp(t.data - m)\n",
    "    s = e / e.sum(axis=dim, keepdims=True)\n",
    "    out = Tensor(s, _children=(t,), _op='softmax')\n",
    "\n",
    "    s = np.moveaxis(s, dim, -1)\n",
    "    shape = s.shape\n",
    "    s = s.reshape(-1, s.shape[-1])\n",
    "\n",
    "    def softmax_backward():\n",
    "        grad = np.zeros_like(s)\n",
    "        for i in range(len(s)):\n",
    "            si = s[i].reshape(1, -1)\n",
    "            j = np.diagflat(si) - si.T@si  # jacobian matrix\n",
    "            grad[i] += j.sum(-1)\n",
    "        grad = grad.reshape(shape)\n",
    "        grad = np.moveaxis(grad, -1, dim)\n",
    "        t.grad += grad\n",
    "        \n",
    "    out._backward = softmax_backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "707efe09-75b6-4d99-a2bb-f002ecd70587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[[ 0., 10.],\n",
       "         [20., 30.]]], dtype=float32)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.arange(4).float().reshape(1, 2, 2).numpy() * 10\n",
    "x = Tensor(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "6187ac40-0743-4482-a4a5-bab5fd33e56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[[2.0611535e-09, 2.0611537e-09],\n",
       "         [1.0000000e+00, 1.0000000e+00]]], dtype=float32)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = softmax(x, -2)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "7e994072-4c6b-453f-8088-c093b34d0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "31a37528-012d-4dbf-9e47-551f456067cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.6379788e-12, -3.6674464e-08],\n",
       "        [-2.0611535e-09,  2.0872665e-08]]], dtype=float32)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3c9dc-c97b-47a4-adcf-97a932846e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745e012-c30a-41f6-992a-0fe3cf737d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
